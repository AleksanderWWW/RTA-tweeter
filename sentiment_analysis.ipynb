{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502127ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Real-time analysis project - tweeter sentiment analysis\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd91e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8694e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================\n",
    "with open(\"config.json\", \"r\", encoding='utf-8') as conf:\n",
    "    config = json.load(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b519efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# TWEET SCRAPING\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d56a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApiConnector:\n",
    "    \"\"\"Object providing methods for tweeter data scraping based on hashtag list provided by user\"\"\"\n",
    "    \n",
    "    url_base = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results={}&tweet.fields=created_at\"\n",
    "    \n",
    "    def __init__(self, hashtags: list, max_results: int, bearer_token: str):\n",
    "        self.hashtags = hashtags\n",
    "        self.max_results = max_results\n",
    "        self.headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    @property\n",
    "    def query(self) -> str:\n",
    "        \n",
    "        _query_list = [\"%23\" + self.hashtags[0]]\n",
    "        \n",
    "        for tag in self.hashtags[1:]:\n",
    "            _query_list.append(\"%20OR%20%23\" + tag)\n",
    "            \n",
    "        _query = \"\".join(_query_list)\n",
    "        return _query\n",
    "        \n",
    "    def get_hashtags(self) -> list:\n",
    "        return self.hashtags\n",
    "    \n",
    "    def set_hashtags(self, hashtags: list) -> None:\n",
    "        self.hashtags = hashtags\n",
    "        \n",
    "    def get_max_results(self) -> int:\n",
    "        return self.max_results\n",
    "        \n",
    "    def set_max_results(max_results: int) -> None:\n",
    "        self.max_results = max_results\n",
    "        \n",
    "    @property\n",
    "    def api_url(self) -> str:\n",
    "        return self.url_base.format(self.query, self.max_results)\n",
    "    \n",
    "    def get_tweets(self) -> list:\n",
    "        \"\"\"Returns a list containing scraped tweets\"\"\"\n",
    "        response = self.session.get(self.api_url, headers=self.headers)\n",
    "        tweets = response.json()[\"data\"]\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac435347",
   "metadata": {},
   "outputs": [],
   "source": [
    "_HASHTAGS = [\"polskilad\", \"polskiwal\", \"nowylad\", \"nowywal\", \"drozyznapis\"]\n",
    "_MAX_RESULTS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add62f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = ApiConnector(_HASHTAGS,\n",
    "                   _MAX_RESULTS,\n",
    "                   config[\"bearer_token\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# SENTIMENT ANALYSIS\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a91778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a33ccd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create spark session\"\"\"\n",
    "\n",
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterAnalysisApp\")\n",
    "# create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# create spark session\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "85bd8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define transformations\"\"\"\n",
    "\n",
    "def get_score(text: str) -> dict:\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "\n",
    "\n",
    "def clean_text(text) -> str:\n",
    "\n",
    "        text=re.sub(r'@[A-Za-z0-9!#$%^&*_]+', '', text)\n",
    "\n",
    "        text=re.sub(r'#', '', text)\n",
    "\n",
    "        text=re.sub(r'RT[\\s]+', '', text)\n",
    "\n",
    "        text=re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "\n",
    "        text=re.sub(r'\\n', '', text) \n",
    "\n",
    "        if text[0] == \":\":\n",
    "            text = text[1:]\n",
    "\n",
    "        return\"\".join([i if ord(i) < 128 else \"\" for i in text])\n",
    "\n",
    "    \n",
    "def leave_char(letter):\n",
    "    return str.isalpha(letter) or letter == \" \"\n",
    "\n",
    "\n",
    "def prepare_text(text):\n",
    "    text_cleaned = clean_text(text)\n",
    "    text_raw = ''.join(filter(leave_char, text_cleaned))\n",
    "    return str(TextBlob(text_raw).translate(from_lang = 'pl', to = 'eng'))    \n",
    "\n",
    "\n",
    "def count_hashtags(text: str) -> dict:\n",
    "    res = {}\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    for word in text.split(\" \"):\n",
    "        if \"#\" in word:\n",
    "            if word.strip() not in res:\n",
    "                res[word.strip()] = 1\n",
    "            else:\n",
    "                res[word.strip()] += 1\n",
    "    return res\n",
    "\n",
    "def score_raw_text(raw_text: str) -> dict:\n",
    "    return get_score(prepare_text(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define operations\"\"\"\n",
    "\n",
    "def tranform_tweets(spark: pyspark.sql.session.SparkSession, tweets: dict) -> pd.DataFrame:\n",
    "    sDf = spark.createDataFrame(tweets, [\"id\", \"timestamp\", \"text\"])\n",
    "    rdd = sDf.rdd\n",
    "    rdd2 = rdd.map(lambda x: (x[\"id\"], x[\"timestamp\"], score_raw_text(x[\"text\"]), count_hashtags(x[\"text\"])))\n",
    "    tempDf = rdd2.toDF([\"timestamp\", \"id\", \"score\", \"hashtags\"])\n",
    "    rdd3 = tempDf.rdd.map(lambda x: (x[\"timestamp\"], x[\"id\"], x[\"score\"], x[\"hashtags\"], len(x[\"hashtags\"])))\n",
    "    final_sDf = rdd3.toDF([\"timestamp\", \"id\", \"score\", \"hashtags\", \"hashtag_count\"])\n",
    "    final_sDf.show()  # display the current batch df\n",
    "    return final_sDf.toPandas()\n",
    "\n",
    "def prepare_tables(pDf: pd.DataFrame) -> tuple:\n",
    "    pDf = pd.concat([pDf, pd.json_normalize(pDf[\"score\"])], axis=1)\n",
    "\n",
    "    # create table for score\n",
    "    score_df = pDf.loc[:, [\"id\", \"timestamp\", \"neg\", \"neu\", \"pos\", \"compound\"]]\n",
    "\n",
    "    # create table for hashtags\n",
    "    hashtag_df = pd.DataFrame([[i, t, k, v, c] for i, t, d, c in pDf[['id', 'timestamp', 'hashtags', 'hashtag_count']].values for k, v in d.items()],\n",
    "    columns=['id','timestamp', 'hashtag_name', 'value', 'hashtag_count'])\n",
    "    hashtag_df.drop(\"value\", axis=1, inplace=True)\n",
    "    \n",
    "    return score_df, hashtag_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# DATABASE\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b14f3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Connect to database\"\"\"\n",
    "from snowflake.sqlalchemy import URL\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f38763a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config[\"database\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b7beac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URL(\n",
    "    user=config[\"user\"],\n",
    "    password=config[\"password\"],\n",
    "    account=config[\"account\"],\n",
    "    warehouse=config[\"warehouse\"],\n",
    "    database=config[\"database\"],\n",
    "    schema =config[\"schema\"]\n",
    "))\n",
    " \n",
    "db_connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "161bad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define operation\"\"\"\n",
    "\n",
    "def send_to_sql(db_connection: sqlalchemy.engine.base.Connection, score_df: pd.DataFrame, hashtag_df: pd.DataFrame) -> None:\n",
    "    try:\n",
    "        # save score to database\n",
    "        score_df.to_sql(\"analiza_sentymentu\", db_connection, if_exists='append', index=False)\n",
    "\n",
    "        # save hashtags to database\n",
    "        hashtag_df.to_sql(\"hashtags\", db_connection, if_exists='append', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"{type(e)}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# MAIN\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a96097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "_TIMEOUT = 10  # time interval between batches\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # while True:\n",
    "        for _ in range(100):\n",
    "            try:\n",
    "                tweets = conn.get_tweets()\n",
    "                pDf = transform_tweets(spark, tweets)\n",
    "                score_df, hashtag_df = prepare_tables(pDf)\n",
    "                send_to_sql(db_connection, score_df, hashtag_df)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"[GLOBAL WHILE] {type(e)}: {str(e)}\")\n",
    "            \n",
    "            finally:\n",
    "                time.sleep(_TIMEOUT)  # no matter what happens, wait before proceeding\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[GLOBAL] {type(e)}: {str(e)}\")\n",
    "        return -1\n",
    "    \n",
    "    finally:\n",
    "        conn.session.close()  # close Api connector session\n",
    "        spark.stop()  # close spark session\n",
    "        sc.stop()  # close spark\n",
    "        db_connection.close()  # close database connection\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cad7df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (6.0.7)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert) (0.4.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert) (3.3.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert) (0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert) (0.1.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert) (5.0.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert) (0.8.4)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert) (2.8.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert) (1.4.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert) (0.5.3)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbconvert) (4.7.1)\n",
      "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.8/dist-packages (from nbconvert) (5.1.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.8/dist-packages (from nbconvert) (2.11.3)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert) (20.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from traitlets>=4.2->nbconvert) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /usr/local/lib/python3.8/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (6.1.12)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.5.1)\n",
      "Requirement already satisfied: async-generator in /usr/local/lib/python3.8/dist-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.10)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.4->nbconvert) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.4->nbconvert) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->bleach->nbconvert) (2.4.7)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (22.0.3)\n",
      "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (2.8.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (45.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert) (0.17.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nbconvert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
