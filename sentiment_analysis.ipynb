{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502127ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Real-time analysis project - tweeter sentiment analysis\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd91e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8694e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================\n",
    "with open(\"config.json\", \"r\", encoding='utf-8') as conf:\n",
    "    config = json.load(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b519efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# TWEET SCRAPING\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d56a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApiConnector:\n",
    "    \"\"\"Object providing methods for tweeter data scraping based on hashtag list provided by user\"\"\"\n",
    "    \n",
    "    url_base = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results={}&tweet.fields=created_at\"\n",
    "    \n",
    "    def __init__(self, hashtags: list, max_results: int, bearer_token: str):\n",
    "        self.hashtags = hashtags\n",
    "        self.max_results = max_results\n",
    "        self.headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    @property\n",
    "    def query(self) -> str:\n",
    "        \n",
    "        _query_list = [\"%23\" + self.hashtags[0]]\n",
    "        \n",
    "        for tag in self.hashtags[1:]:\n",
    "            _query_list.append(\"%20OR%20%23\" + tag)\n",
    "            \n",
    "        _query = \"\".join(_query_list)\n",
    "        return _query\n",
    "        \n",
    "    def get_hashtags(self) -> list:\n",
    "        return self.hashtags\n",
    "    \n",
    "    def set_hashtags(self, hashtags: list) -> None:\n",
    "        self.hashtags = hashtags\n",
    "        \n",
    "    def get_max_results(self) -> int:\n",
    "        return self.max_results\n",
    "        \n",
    "    def set_max_results(max_results: int) -> None:\n",
    "        self.max_results = max_results\n",
    "        \n",
    "    @property\n",
    "    def api_url(self) -> str:\n",
    "        return self.url_base.format(self.query, self.max_results)\n",
    "    \n",
    "    def get_tweets(self) -> list:\n",
    "        \"\"\"Returns a list containing scraped tweets\"\"\"\n",
    "        response = self.session.get(self.api_url, headers=self.headers)\n",
    "        tweets = response.json()[\"data\"]\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b0ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# SENTIMENT ANALYSIS\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a91778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85bd8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define transformations\"\"\"\n",
    "\n",
    "def get_score(text: str) -> dict:\n",
    "    return SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "\n",
    "\n",
    "def clean_text(text) -> str:\n",
    "\n",
    "        text=re.sub(r'@[A-Za-z0-9!#$%^&*_]+', '', text)\n",
    "\n",
    "        text=re.sub(r'#', '', text)\n",
    "\n",
    "        text=re.sub(r'RT[\\s]+', '', text)\n",
    "\n",
    "        text=re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "\n",
    "        text=re.sub(r'\\n', '', text) \n",
    "\n",
    "        if text[0] == \":\":\n",
    "            text = text[1:]\n",
    "\n",
    "        return\"\".join([i if ord(i) < 128 else \"\" for i in text])\n",
    "\n",
    "    \n",
    "def leave_char(letter):\n",
    "    return str.isalpha(letter) or letter == \" \"\n",
    "\n",
    "\n",
    "def prepare_text(text):\n",
    "    text_cleaned = clean_text(text)\n",
    "    text_raw = ''.join(filter(leave_char, text_cleaned))\n",
    "    return str(TextBlob(text_raw).translate(from_lang = 'pl', to = 'eng'))    \n",
    "\n",
    "\n",
    "def count_hashtags(text: str) -> dict:\n",
    "    res = {}\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    for word in text.split(\" \"):\n",
    "        if \"#\" in word:\n",
    "            if word.strip() not in res:\n",
    "                res[word.strip()] = 1\n",
    "            else:\n",
    "                res[word.strip()] += 1\n",
    "    return res\n",
    "\n",
    "def score_raw_text(raw_text: str) -> dict:\n",
    "    return get_score(prepare_text(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e93ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define operations\"\"\"\n",
    "\n",
    "def transform_tweets(spark, tweets: dict) -> pd.DataFrame:\n",
    "    sDf = spark.createDataFrame(tweets, [\"id\", \"timestamp\", \"text\"])\n",
    "    rdd = sDf.rdd\n",
    "    rdd2 = rdd.map(lambda x: (x[\"id\"], x[\"timestamp\"], score_raw_text(x[\"text\"]), count_hashtags(x[\"text\"])))\n",
    "    tempDf = rdd2.toDF([\"timestamp\", \"id\", \"score\", \"hashtags\"])\n",
    "    rdd3 = tempDf.rdd.map(lambda x: (x[\"timestamp\"], x[\"id\"], x[\"score\"], x[\"hashtags\"], len(x[\"hashtags\"])))\n",
    "    final_sDf = rdd3.toDF([\"timestamp\", \"id\", \"score\", \"hashtags\", \"hashtag_count\"])\n",
    "    final_sDf.show()  # display the current batch df\n",
    "    return final_sDf.toPandas()\n",
    "\n",
    "def prepare_tables(pDf: pd.DataFrame) -> tuple:\n",
    "    pDf = pd.concat([pDf, pd.json_normalize(pDf[\"score\"])], axis=1)\n",
    "\n",
    "    # create table for score\n",
    "    score_df = pDf.loc[:, [\"id\", \"timestamp\", \"neg\", \"neu\", \"pos\", \"compound\"]]\n",
    "\n",
    "    # create table for hashtags\n",
    "    hashtag_df = pd.DataFrame([[i, t, k, v, c] for i, t, d, c in pDf[['id', 'timestamp', 'hashtags', 'hashtag_count']].values for k, v in d.items()],\n",
    "    columns=['id','timestamp', 'hashtag_name', 'value', 'hashtag_count'])\n",
    "    hashtag_df.drop(\"value\", axis=1, inplace=True)\n",
    "    \n",
    "    return score_df, hashtag_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54e7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# DATABASE\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b14f3b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Connect to database\"\"\"\n",
    "from snowflake.sqlalchemy import URL\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "161bad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define operation\"\"\"\n",
    "\n",
    "def send_to_sql(db_connection, score_df: pd.DataFrame, hashtag_df: pd.DataFrame) -> None:\n",
    "    try:\n",
    "        # save score to database\n",
    "        score_df.to_sql(\"analiza_sentymentu\", db_connection, if_exists='append', index=False)\n",
    "\n",
    "        # save hashtags to database\n",
    "        hashtag_df.to_sql(\"hashtags\", db_connection, if_exists='append', index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print.error(f\"{type(e)}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5703c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# MAIN\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da8abf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "_TIMEOUT = 10  # time interval between batches\n",
    "_HASHTAGS = [\"polskilad\", \"polskiwal\", \"nowylad\", \"nowywal\", \"drozyznapis\"]\n",
    "_BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "def main():\n",
    "    flag = 0\n",
    "    \n",
    "    print(\"starting process\")\n",
    "    \n",
    "    try:\n",
    "        conn = ApiConnector(_HASHTAGS,\n",
    "                       _BATCH_SIZE,\n",
    "                       config[\"bearer_token\"])\n",
    "        print(\"api connector instantiated\")\n",
    "\n",
    "        # create spark configuration\n",
    "        conf = SparkConf()\n",
    "        conf.setAppName(\"TwitterAnalysisApp\")\n",
    "        # create spark context with the above configuration\n",
    "        sc = SparkContext(conf=conf)\n",
    "        sc.setLogLevel(\"ERROR\")\n",
    "        # create spark session\n",
    "        spark = SparkSession(sc)\n",
    "        print(\"spark session created\")\n",
    "\n",
    "        db_config = config[\"database\"]\n",
    "\n",
    "        engine = create_engine(URL(\n",
    "            user=db_config[\"user\"],\n",
    "            password=db_config[\"password\"],\n",
    "            account=db_config[\"account\"],\n",
    "            warehouse=db_config[\"warehouse\"],\n",
    "            database=db_config[\"database\"],\n",
    "            schema=db_config[\"schema\"]\n",
    "        ))\n",
    "\n",
    "        db_connection = engine.connect()\n",
    "        print(\"connected to database\")\n",
    "    \n",
    "        # while True:\n",
    "        for _ in range(2):\n",
    "            try:\n",
    "                tweets = conn.get_tweets()\n",
    "                pDf = transform_tweets(spark, tweets)\n",
    "                score_df, hashtag_df = prepare_tables(pDf)\n",
    "                send_to_sql(db_connection, score_df, hashtag_df)\n",
    "                print(\"data sent\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"[GLOBAL WHILE] {type(e)}: {str(e)}\")\n",
    "            \n",
    "            finally:\n",
    "                time.sleep(_TIMEOUT)  # no matter what happens, wait before proceeding\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"[GLOBAL] {type(e)}: {str(e)}\")\n",
    "        flag = -1\n",
    "    \n",
    "    finally:\n",
    "        print(\"tearing down\")\n",
    "        conn.session.close()  # close Api connector session\n",
    "        spark.stop()  # close spark session\n",
    "        sc.stop()  # close spark\n",
    "        db_connection.close()  # close database connection\n",
    "        print(\"process finished with code: %d\" % flag)\n",
    "        \n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51a60853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+-------------+\n",
      "|           timestamp|                 id|               score|            hashtags|hashtag_count|\n",
      "+--------------------+-------------------+--------------------+--------------------+-------------+\n",
      "|2022-01-17T15:07:...|1483093618260946949|{neg -> 0.0, pos ...|                  {}|            0|\n",
      "|2022-01-17T15:07:...|1483093603652374529|{neg -> 0.0, pos ...|     {#nowywał -> 1}|            1|\n",
      "|2022-01-17T15:07:...|1483093554612289537|{neg -> 0.0, pos ...|   {#PolskiŁad -> 1}|            1|\n",
      "|2022-01-17T15:07:...|1483093529765330946|{neg -> 0.0, pos ...|{#Katowice: -> 1,...|            2|\n",
      "|2022-01-17T15:06:...|1483093497280450561|{neg -> 0.109, po...|   {#Katowice: -> 1}|            1|\n",
      "|2022-01-17T15:06:...|1483093477969825794|{neg -> 0.0, pos ...|   {#PolskiWał -> 1}|            1|\n",
      "|2022-01-17T15:06:...|1483093410030530565|{neg -> 0.0, pos ...|{#EstońskiCIT -> ...|            2|\n",
      "|2022-01-17T15:06:...|1483093397908951058|{neg -> 0.0, pos ...|{#K… -> 1, #Polsk...|            2|\n",
      "|2022-01-17T15:06:...|1483093383950307331|{neg -> 0.132, po...|                  {}|            0|\n",
      "|2022-01-17T15:06:...|1483093379042918402|{neg -> 0.237, po...|                  {}|            0|\n",
      "+--------------------+-------------------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/snowflake/sqlalchemy/snowdialect.py:206: SAWarning: Dialect snowflake:snowflake will not make use of SQL compilation caching as it does not set the 'supports_statement_cache' attribute to ``True``.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Dialect maintainers should seek to set this attribute to True after appropriate development and testing for SQLAlchemy 1.4 caching support.   Alternatively, this attribute may be set to False which will disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n",
      "  results = connection.execute(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data sent\n",
      "+--------------------+-------------------+--------------------+--------------------+-------------+\n",
      "|           timestamp|                 id|               score|            hashtags|hashtag_count|\n",
      "+--------------------+-------------------+--------------------+--------------------+-------------+\n",
      "|2022-01-17T15:07:...|1483093618260946949|{neg -> 0.0, pos ...|                  {}|            0|\n",
      "|2022-01-17T15:07:...|1483093603652374529|{neg -> 0.0, pos ...|     {#nowywał -> 1}|            1|\n",
      "|2022-01-17T15:07:...|1483093554612289537|{neg -> 0.0, pos ...|   {#PolskiŁad -> 1}|            1|\n",
      "|2022-01-17T15:07:...|1483093529765330946|{neg -> 0.0, pos ...|{#Katowice: -> 1,...|            2|\n",
      "|2022-01-17T15:06:...|1483093497280450561|{neg -> 0.109, po...|   {#Katowice: -> 1}|            1|\n",
      "|2022-01-17T15:06:...|1483093477969825794|{neg -> 0.0, pos ...|   {#PolskiWał -> 1}|            1|\n",
      "|2022-01-17T15:06:...|1483093410030530565|{neg -> 0.0, pos ...|{#EstońskiCIT -> ...|            2|\n",
      "|2022-01-17T15:06:...|1483093397908951058|{neg -> 0.0, pos ...|{#K… -> 1, #Polsk...|            2|\n",
      "|2022-01-17T15:06:...|1483093383950307331|{neg -> 0.132, po...|                  {}|            0|\n",
      "|2022-01-17T15:06:...|1483093379042918402|{neg -> 0.237, po...|                  {}|            0|\n",
      "+--------------------+-------------------+--------------------+--------------------+-------------+\n",
      "\n",
      "data sent\n",
      "tearing down\n",
      "process finished with code: 0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a12f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
